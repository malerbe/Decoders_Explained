{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bb8a978",
   "metadata": {},
   "source": [
    "Understanding decoders is a key point to understand how transformers work. In this notebook, we will present what encoders are and why they are useful by themselves and inside more complex architectures like the famous transformer architecture. \n",
    "\n",
    "*Note : It is considered necessary to have the knowledge linked to encoders before reading this. Most information given in my Encoder_Explained repository won't be given again in this one*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee47b7f",
   "metadata": {},
   "source": [
    "# I. How do decoders work ? \n",
    "\n",
    "## 1. What are decoders\n",
    "\n",
    "Just like encoders, decoders can be used as a standalone architecture for different tasks. One may use decoders for the same purpose as encoders, albeit with generally a loss of performance. Let's see what are the architectural differences between encoders and decoders:\n",
    "\n",
    "Again, let's use the example of the sentence \"the cat likes cheese\".\n",
    "\n",
    "Passing them through the decoder will give you a numerical representation of each word. Just like encoders, it gives you a feature vector for each of these words. \n",
    "\n",
    "## 2. Main difference with encoders: the masked self-attention mechanism\n",
    "\n",
    "The difference here, is that the decoder doesn't use the self-attention mechanism the same way: it uses **masked self-attention**.\n",
    "\n",
    "This difference can be illustrated with the word cat for example:\n",
    "\n",
    "The returned feature vector of this word will only be affected by the word the. The next elements of the sequence are not used by the decoder to make the representation of the element. The next elements are said to be \"masked\". \n",
    "\n",
    "Actually, this is more of an example than what it is in reality, to be more correct, we should say that while encoders have access to a bi-directional context, decoders have access only to context from the left or from the right: it's a unidirectional context. \n",
    "\n",
    "## 3. In what cases are decoders helpful ?\n",
    "\n",
    "Decoders can  be used as standalone models in a variety of tasks:\n",
    "\n",
    "- Causal tasks; Generating sequences (example: GPT-2)\n",
    "\n",
    "\n",
    "In general, decoders are useful for tasks linked to the need of a unidirectional extraction of meaningful information in a sequence. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f800a859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4636b31",
   "metadata": {},
   "source": [
    "## Sources:\n",
    "\n",
    "\"Transformer: decodeur\", Hugging Face Youtube channel (https://www.youtube.com/watch?v=d_ixlCubqQw)\n",
    "\n",
    "\"A Dive Into Multihead Attention, Self-Attention and Cross-Attention\", Machine Learning Studio Youtube channel (https://www.youtube.com/watch?v=mmzRYGCfTzc)\n",
    "\n",
    "\"Attention Is All You Need\", Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin(arXiv:1706.03762)\n",
    "\n",
    "\"Self-Attention Using Scaled Dot-Product Approach\", Machine Learning Studio Youtube channel (https://youtu.be/1IKrHh2X0F0?si=fQozjbfBRPw7J9p9)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
