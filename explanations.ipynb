{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bb8a978",
   "metadata": {},
   "source": [
    "Understanding decoders is a key point to understand how transformers work. In this notebook, we will present what encoders are and why they are useful by themselves and inside more complex architectures like the famous transformer architecture. \n",
    "\n",
    "*Note : It is considered necessary to have the knowledge linked to encoders before reading this. Most information given in my Encoders_Explained repository won't be given again in this one*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee47b7f",
   "metadata": {},
   "source": [
    "# I. How do decoders work ? \n",
    "\n",
    "## 1. What are decoders\n",
    "\n",
    "Just like encoders, decoders can be used as a standalone architecture for different tasks. One may use decoders for the same purpose as encoders, albeit with generally a loss of performance. On the contrary, decoders are much more adapted for certain tasks like generating text. Let's see what are the architectural differences between encoders and decoders:\n",
    "\n",
    "Again, let's use the example of the sentence \"the cat likes cheese\".\n",
    "\n",
    "Passing them through the decoder will give you a numerical representation of each word. Just like encoders, it gives you a feature vector for each of these words. \n",
    "\n",
    "## 2. Main difference with encoders: the masked self-attention mechanism\n",
    "\n",
    "The difference here, is that the decoder doesn't use the self-attention mechanism the same way: it uses **masked self-attention**.\n",
    "\n",
    "This difference can be illustrated with the word cat for example:\n",
    "\n",
    "The returned feature vector of this word will only be affected by the word the. The next elements (i.e. future elements) of the sequence are not used by the decoder to make the representation of the element. The next elements are said to be \"masked\". \n",
    "\n",
    "Actually, this is more of an example than what it is in reality, to be more correct, we should say that while encoders have access to a bi-directional context, decoders have access only to context from the left or from the right: it's a unidirectional context. \n",
    "\n",
    "## 3. In what cases are decoders helpful ?\n",
    "\n",
    "Decoders can  be used as standalone models in a variety of tasks:\n",
    "\n",
    "- Causal tasks; Generating sequences (example: GPT-2)\n",
    "- Language modeling\n",
    "- Text generation\n",
    "\n",
    "\n",
    "In general, decoders are useful for tasks linked to the need of a unidirectional extraction of meaningful information in a sequence. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b851286",
   "metadata": {},
   "source": [
    "# II. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48650fc",
   "metadata": {},
   "source": [
    "As done in the *Encoders_Explained* repo, we will implement the decoder architecture following the \"Attention is all you need\" paper:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"assets/decoder_architecture_diagram.png\" alt=\"Architecture\" width=\"300\">\n",
    "</div>\n",
    "\n",
    "## 1. Multi-Head Attention\n",
    "\n",
    "Cf Encoder repo to get knowledge about it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f800a859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim,\n",
    "                 num_heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        assert (self.head_dim * num_heads == embed_dim), \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.V = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.K = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.Q = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "        self.fc_out = nn.Linear(num_heads * self.head_dim, embed_dim)\n",
    "\n",
    "    def forward(self,\n",
    "                query,\n",
    "                keys,\n",
    "                values,\n",
    "                mask=None):\n",
    "        \n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "        \n",
    "        # 1. Extract the embeddings from the input:\n",
    "        Q = self.Q(query) # [N, query_len, embed_dim]\n",
    "        K = self.K(keys) # [N, key_len, embed_dim]\n",
    "        V = self.V(values) # [N, value_len, embed_dim]\n",
    "\n",
    "        # 2. Split embeddings into multiple heads\n",
    "        Queries = Q.reshape(N, query_len, self.num_heads, self.head_dim) # [N, query_len, num_heads, head_dim]\n",
    "        Keys = K.reshape(N, key_len, self.num_heads, self.head_dim) # [N, key_len, num_heads, head_dim]\n",
    "        Values = V.reshape(N, value_len, self.num_heads, self.head_dim) # [N, value_len, num_heads, head_dim]\n",
    "\n",
    "        # 3. Compute the attention scores\n",
    "        # matmul\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [Queries, Keys])\n",
    "\n",
    "        # scale\n",
    "        energy = energy / (self.embed_dim ** (1/2)) # Explanations https://youtu.be/1IKrHh2X0F0?si=fQozjbfBRPw7J9p9\n",
    "        \n",
    "        # apply mask\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "        \n",
    "        # apply softmax to get attention weights\n",
    "        attention = torch.softmax(energy, dim=3)\n",
    "\n",
    "        # final matmul between attention weights with values\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, Values]).reshape(N, query_len, self.num_heads * self.head_dim) # [N, query_len, num_heads, head_dim]\n",
    "\n",
    "        # Out shape :       (N, query_len, num_heads, head_dim) after einsum and flattening the last two dimensions\n",
    "\n",
    "        # Final linear layer\n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        \n",
    "        return out \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5efa32",
   "metadata": {},
   "source": [
    "## 2. Transformer Block\n",
    "\n",
    "Same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49e5b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, forward_expansion * embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "\n",
    "        attention = self.attention(query, key, value, mask)\n",
    "        x = self.dropout(self.norm1(attention + query)) # Residual connection is done with just the query input\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9332c67",
   "metadata": {},
   "source": [
    "## 3. Decoder Block\n",
    "\n",
    "\n",
    "Last time, implementing the Transformer Block to make the Encoder was enough. But the Decoder architecture is naturally different (as we don't expect the exact same behaviour from two different things of course).\n",
    "\n",
    "The Decoder architecture involved the \"usual\" Transformer Block implemented just before, plus another self-attention. THe Transformer Block would take, in the context. of a whole transformer, entries from both :\n",
    "\n",
    "1. The embedding processed by the first Masked Multi-Head Attention (which means represention from only previous/past elements in the sequence)\n",
    "\n",
    "2. The embeddings processed by the encoder, which are embeddings representative of the previous (if masked used in the encoder) words (or more generaly elements) as part of the whole sub-sequence. \n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"assets/decoder_architecture_diagram.png\" alt=\"Architecture\" width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17ce99b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "               embed_size,\n",
    "            num_heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            device):\n",
    "\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.attention = SelfAttention(embed_size, num_heads)\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.transformer_block = TransformerBlock(\n",
    "            embed_size,\n",
    "            num_heads,\n",
    "            dropout=dropout,\n",
    "            forward_expansion=forward_expansion\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "\n",
    "    def forward(self, x, value, key, src_mask, trg_mask):\n",
    "        attention = self.attention(x, x, x, trg_mask)\n",
    "        x = self.dropout(self.norm(attention + x))\n",
    "        out = self.transformer_block(value, key, x, src_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd72123",
   "metadata": {},
   "source": [
    "In the case of using the decoder as a standalone model for generation (without an encoder), we will use this class instead: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efee58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandaloneDecoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "               embed_size,\n",
    "            num_heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            device):\n",
    "\n",
    "        super(StandaloneDecoderBlock, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.attention = SelfAttention(embed_size, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "\n",
    "    def forward(self, x, src_mask, trg_mask):\n",
    "        attention = self.attention(x, x, x, trg_mask)\n",
    "        x = self.dropout(self.norm1(attention + x))\n",
    "\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1118a1eb",
   "metadata": {},
   "source": [
    "which simply corresponds to our previous decoder class, except this one doesn't use a second full transformer block, but just its feed-forward block instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b160cd",
   "metadata": {},
   "source": [
    "## 4. Decoder\n",
    "\n",
    "The decoder architecture is very simply implemented using the pre-made blocks. The overall implementation is really done by the exact same process as for the encoder. \n",
    "\n",
    "**Just be attentive to the connexion taken from the encoder in the context of a whole transformer architecture, with the enc-out variable taken as an input by the forward method**\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"assets/decoder.png\" alt=\"Scheme\" width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0becdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                trg_vocab_size,\n",
    "                embed_size,\n",
    "                num_layers,\n",
    "                num_heads,\n",
    "                forward_expansion,\n",
    "                dropout,\n",
    "                device,\n",
    "                max_length):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(\n",
    "                    embed_size,\n",
    "                    num_heads,\n",
    "                    forward_expansion,\n",
    "                    dropout,\n",
    "                    device\n",
    "                ) for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask, trg_mask): #enc_out = encoder output\n",
    "        N, self.seq_length = x.shape\n",
    "        positions = torch.arange(0, self.seq_length).expand(N, self.seq_length).to(self.device)\n",
    "        x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
    "\n",
    "        out = self.fc_out(x)\n",
    "\n",
    "        return out  # [N, seq_length, trg_vocab_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dceca5d5",
   "metadata": {},
   "source": [
    "## 5. Practical case\n",
    "\n",
    "In the mini-project, we will detail how to use decoder as a standalone model for a generation task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4636b31",
   "metadata": {},
   "source": [
    "## Sources:\n",
    "\n",
    "\"Transformer: decodeur\", Hugging Face Youtube channel (https://www.youtube.com/watch?v=d_ixlCubqQw)\n",
    "\n",
    "\"A Dive Into Multihead Attention, Self-Attention and Cross-Attention\", Machine Learning Studio Youtube channel (https://www.youtube.com/watch?v=mmzRYGCfTzc)\n",
    "\n",
    "\"Attention Is All You Need\", Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin(arXiv:1706.03762)\n",
    "\n",
    "\"Self-Attention Using Scaled Dot-Product Approach\", Machine Learning Studio Youtube channel (https://youtu.be/1IKrHh2X0F0?si=fQozjbfBRPw7J9p9)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
