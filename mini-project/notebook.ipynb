{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6494ae95",
   "metadata": {},
   "source": [
    "# Encoders mini-project:\n",
    "## Implementation of a numerical sequence generator\n",
    "\n",
    "### I. Introduction\n",
    "\n",
    "As seen in the explanation.ipnyb notebook, enven though decoders as implemented in this repository were introduced as part of a bigger architecture - the *transformer* architecture - they can be used as a standalone architecture for sequences generation. \n",
    "\n",
    "In this notebook, we will implement a simple sequence generator and make different tests and observation to illustrate what was said in the explanations. \n",
    "\n",
    "### II. Implementation of the model\n",
    "\n",
    "First, let's import the code from model.py. The containt of this file if precisely what was done in the explanations notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "16eeb3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import SelfAttention, TransformerBlock, StandaloneDecoderBlock, StandaloneDecoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58477d55",
   "metadata": {},
   "source": [
    "### III. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4e7d65be",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d11c3e6",
   "metadata": {},
   "source": [
    "\n",
    "### IV. Creation of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fb17e0",
   "metadata": {},
   "source": [
    "For this mini-project, we will make the dataset ourself so that the data is both very simple, in on-demand quantities and completely mastered. We will make the ArithmeticSequenceDataset so that we will be able to generate two simple different types of arithmetic sequences:\n",
    "\n",
    "Arithmetic: $U_n = a + nb$\n",
    "\n",
    "Geometric: $U_n = a(b)^n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "7dde4183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 6, 10,  1,  2, 10,  2,  4, 10,  4,  8, 10,  9,  6, 10,  1,  9,  2, 12,\n",
       "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]),\n",
       " tensor([ 4,  8, 10,  9,  6, 10,  1,  9,  2, 12, 11, 11, 11, 11, 11, 11, 11, 11,\n",
       "         11, 11, 11, 11]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import random as rd\n",
    "import numpy as np\n",
    "\n",
    "class ArithmeticSequenceDataset(Dataset):\n",
    "    def __init__(self, size=1000, length=6, proportions=(50,50), max_length=30):\n",
    "        super().__init__()\n",
    "\n",
    "        self.size = size\n",
    "        self.length = length\n",
    "        self.proportions = proportions\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.max_value = self.make_sequences()\n",
    "\n",
    "        # We could make different choices for the vocabulary, but we will choose to limit as\n",
    "        # much as possible its size\n",
    "        self.vocab = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "                      '<SEP>',\n",
    "                      '<PAD>',\n",
    "                      '<EOS>']\n",
    "\n",
    "        self.vocab2idx = {c: i for i, c in enumerate(self.vocab)}\n",
    "        self.idx2vocab = {i: c for c, i in self.vocab2idx.items()}\n",
    "\n",
    "        # Convert sequences to tokens\n",
    "        self.tokenized_sequences = [self.tokenize_sequence(seq) for seq in self.sequences]\n",
    "\n",
    "    def tokenize_sequence(self, sequence):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            sequence (list): example: [1, 23, 456]\n",
    "          \n",
    "        Returns:\n",
    "            (list): example: ['1', '<SEP>', '2', '3', '<SEP>', '4', '5', '6', '<EOS>']\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        \n",
    "        for i, num in enumerate(sequence):\n",
    "            for digit in str(num):\n",
    "                tokens.append(digit)\n",
    "            \n",
    "            if i < len(sequence) - 1:\n",
    "                tokens.append('<SEP>')\n",
    "        \n",
    "        tokens.append('<EOS>')\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def detokenize_sequence(self, tokens):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            sequence (list): example: ['1', '<SEP>', '2', '3', '<SEP>', '4', '5', '6', '<EOS>']\n",
    "          \n",
    "        Returns:\n",
    "            (list): example: [1, 23, 456]\n",
    "        \"\"\"\n",
    "\n",
    "        numbers = []\n",
    "        current_number = \"\"\n",
    "        \n",
    "        for token in tokens:\n",
    "            if token in ['<SEP>', '<EOS>', '<PAD>']:\n",
    "                if current_number:\n",
    "                    numbers.append(int(current_number))\n",
    "                    current_number = \"\"\n",
    "                if token == '<EOS>':\n",
    "                    break\n",
    "            elif token.isdigit():\n",
    "                current_number += token\n",
    "        \n",
    "        if current_number:\n",
    "            numbers.append(int(current_number))\n",
    "            \n",
    "        return numbers\n",
    "\n",
    "    def generate_random_arithmetic_sequence(self):\n",
    "        \"\"\"\n",
    "        Makes an array of size length containing the length first values\n",
    "        for U_n = a + b*n with a and b chosen randomly\n",
    "\n",
    "        Input:\n",
    "            length: number of values generated\n",
    "\n",
    "        Returns:\n",
    "            (np.array)\n",
    "        \"\"\"\n",
    "\n",
    "        a = rd.randint(0, 5)\n",
    "        b = rd.randint(1, 5)\n",
    "\n",
    "        _seq = [] \n",
    "        for n in range(self.length):\n",
    "\n",
    "            _seq.append(a + n*b)\n",
    "\n",
    "        return np.array(_seq)\n",
    "    \n",
    "    def generate_random_geometric_sequence(self):\n",
    "        \"\"\"\n",
    "        Makes an array of size length containing the length first values\n",
    "        for U_n = a*b^n with a and b chosen randomly\n",
    "\n",
    "        Input:\n",
    "            length: number of values generated\n",
    "\n",
    "        Returns:\n",
    "            (np.array)\n",
    "        \"\"\"\n",
    "\n",
    "        a = rd.randint(1, 4)\n",
    "        b = rd.randint(2, 4)\n",
    "\n",
    "        _seq = [] \n",
    "        for n in range(1, self.length+1):\n",
    "\n",
    "            _seq.append(a*(b**n))\n",
    "\n",
    "        return np.array(_seq)\n",
    "    \n",
    "    def make_sequences(self):\n",
    "        self.sequences = []\n",
    "\n",
    "        num_arithmetic = self.size*self.proportions[0]//100\n",
    "        num_geometric = self.size*self.proportions[1]//100\n",
    "\n",
    "        max_value = 0\n",
    "\n",
    "        # Generate the arithmetic sequences:\n",
    "        for k in range(num_arithmetic):\n",
    "            generated_seq = self.generate_random_arithmetic_sequence()\n",
    "            self.sequences.append(generated_seq)\n",
    "\n",
    "            if generated_seq[-1] > max_value:\n",
    "                max_value = generated_seq[-1]\n",
    "\n",
    "        # Generate the geometric sequences:\n",
    "        for k in range(num_geometric):\n",
    "            generated_seq = self.generate_random_geometric_sequence()\n",
    "            self.sequences.append(generated_seq)\n",
    "\n",
    "            if generated_seq[-1] > max_value:\n",
    "                max_value = generated_seq[-1]\n",
    "\n",
    "        self.sequences = np.array(self.sequences)\n",
    "\n",
    "        return max_value\n",
    "    \n",
    "    def pad_sequence(self, tokens):\n",
    "        \"\"\"Pad sequence to max_length\"\"\"\n",
    "        if len(tokens) > self.max_length:\n",
    "            return tokens[:self.max_length]  # Truncate\n",
    "        \n",
    "        # Pad\n",
    "        padded = tokens + ['<PAD>'] * (self.max_length - len(tokens))\n",
    "        return padded\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]  # [2, 4, 8, 16, 32]\n",
    "        \n",
    "        # Context size to decide how much context is needed.\n",
    "        # In our, case, we need the first 3 numbers + 3 seperators\n",
    "        context_size = 3  # Minimum to get the pattern\n",
    "        predict_size = len(sequence) - context_size\n",
    "        \n",
    "        # Split context and target\n",
    "        context = sequence[:context_size]    # [2, 4, 8]\n",
    "        target = sequence[context_size:]     # [16, 32]\n",
    "        \n",
    "        # Tokenize\n",
    "        context_tokens = self.tokenize_sequence(context.tolist())[:-1] + ['<SEP>'] # ['2', '<SEP>', '4', '<SEP>', '8']\n",
    "        target_tokens = self.tokenize_sequence(target.tolist()) # ['<SEP>', '8', '<SEP>', '1', '6', '<SEP>', '3', '2', '<EOS>']\n",
    "        \n",
    "        full_sequence = context_tokens + target_tokens\n",
    "        \n",
    "        # Pad to max_length\n",
    "        padded_sequence = self.pad_sequence(full_sequence)\n",
    "\n",
    "        context_len = len(context_tokens)\n",
    "        \n",
    "        # Create input and target tensors (shifted by 1)\n",
    "        input_ids = torch.tensor([self.vocab2idx[token] for token in padded_sequence[:-1]], dtype=torch.long)\n",
    "        target_ids = torch.tensor([self.vocab2idx[token] for token in padded_sequence[context_len:]], dtype=torch.long)\n",
    "        \n",
    "        # Create loss mask (only predict on target tokens, not context)\n",
    "        \n",
    "        loss_mask = torch.cat([\n",
    "            torch.zeros(context_len, dtype=torch.float),  # Don't compute loss on context\n",
    "            torch.ones(len(target_tokens), dtype=torch.float),  # Compute loss on predictions\n",
    "            torch.zeros(self.max_length - len(full_sequence), dtype=torch.float)  # Don't compute loss on padding\n",
    "        ])\n",
    "        \n",
    "        return input_ids, target_ids, loss_mask\n",
    "\n",
    "\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "\n",
    "test = ArithmeticSequenceDataset()\n",
    "test.__getitem__(999)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713c4694",
   "metadata": {},
   "source": [
    "We will also need to implement the associated dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "03d399be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = ArithmeticSequenceDataset(\n",
    "    size=2000,\n",
    "    length=6,\n",
    "    proportions=(50, 50)\n",
    ")\n",
    "\n",
    "test_dataset = ArithmeticSequenceDataset(\n",
    "    size=400,\n",
    "    length=6,\n",
    "    proportions=(50, 50)\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
