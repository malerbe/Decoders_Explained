{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6494ae95",
   "metadata": {},
   "source": [
    "# Encoders mini-project:\n",
    "## Implementation of a numerical sequence generator\n",
    "\n",
    "### I. Introduction\n",
    "\n",
    "As seen in the explanation.ipnyb notebook, enven though decoders as implemented in this repository were introduced as part of a bigger architecture - the *transformer* architecture - they can be used as a standalone architecture for sequences generation. \n",
    "\n",
    "In this notebook, we will implement a simple sequence generator and make different tests and observation to illustrate what was said in the explanations. \n",
    "\n",
    "### II. Implementation of the model\n",
    "\n",
    "First, let's import the code from model.py. The containt of this file if precisely what was done in the explanations notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "16eeb3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import SelfAttention, TransformerBlock, StandaloneDecoderBlock, StandaloneDecoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58477d55",
   "metadata": {},
   "source": [
    "### III. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7d65be",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 100\n",
    "EMBED_SIZE = 256\n",
    "NUM_LAYERS = 4\n",
    "NUM_HEADS = 4\n",
    "FORWARD_EXPANSION = 4\n",
    "LEARNING_RATE = 5e-4\n",
    "DROPOUT = 0.2\n",
    "MAX_LENGTH=30\n",
    "PAD_TOKEN_ID=11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d11c3e6",
   "metadata": {},
   "source": [
    "\n",
    "### IV. Creation of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fb17e0",
   "metadata": {},
   "source": [
    "For this mini-project, we will make the dataset class ourself, using the data from https://gist.github.com/elifiner/cc90fdd387449158829515782936a9a4 \n",
    "\n",
    "This modified file text is available in the same folder as this notebook. Names with accents were removed for most as well as unvalid entries for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7dde4183",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import random as rd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "class NamesDataset(Dataset):\n",
    "    def __init__(self, path_to_names=\"first-names.txt\", max_length=20, train=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_length = max_length\n",
    "\n",
    "        with open(path_to_names, 'r', encoding='utf-8') as f:\n",
    "            fcontent = f.read()\n",
    "\n",
    "        self.names = fcontent.split(\"\\n\")\n",
    "\n",
    "        # For this specific datasrt, need to clean some names:\n",
    "        self.names = [name for name in self.names if not '.' in name]\n",
    "        self.names = [name for name in self.names if not '2' in name]\n",
    "\n",
    "        rd.seed(24)\n",
    "        rd.shuffle(self.names)\n",
    "        \n",
    "        if train:\n",
    "            self.names = self.names[:len(self.names)*80//100]\n",
    "        else:\n",
    "            self.names = self.names[len(self.names)*80//100:]\n",
    "\n",
    "        self.vocab = list(string.ascii_lowercase) + ['√©', '√†'] + ['<PAD>', '<SOS>', '<EOS>']\n",
    "\n",
    "        self.vocab2idx = {token: i for i, token in enumerate(self.vocab)}\n",
    "        self.idx2vocab = {i: token for token, i in self.vocab2idx.items()}\n",
    "\n",
    "        self.pad_idx = self.vocab2idx['<PAD>']\n",
    "        self.sos_idx = self.vocab2idx['<SOS>']  \n",
    "        self.eos_idx = self.vocab2idx['<EOS>']\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "    def decode(self, indices):\n",
    "        if torch.is_tensor(indices):\n",
    "            indices = indices.tolist()\n",
    "        \n",
    "        chars = []\n",
    "        for idx in indices:\n",
    "            char = self.idx2vocab[idx]\n",
    "            if char == '<EOS>' or char == '<PAD>':\n",
    "                break\n",
    "            if char != '<SOS>':\n",
    "                chars.append(char)\n",
    "        return ''.join(chars)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        name = self.names[idx].lower()\n",
    "\n",
    "        # Tokenize\n",
    "        tokenized_name = [self.vocab2idx[char] for char in name]\n",
    "\n",
    "        tokenized_seq = [self.vocab2idx['<SOS>']]+ tokenized_name + [self.vocab2idx['<EOS>']]\n",
    "        \n",
    "        # Pad\n",
    "        tokenized_seq.extend([self.vocab2idx['<PAD>']] * (self.max_length-len(tokenized_seq)))\n",
    "\n",
    "        # Auto-regressor:\n",
    "\n",
    "        \n",
    "        input_ids = torch.tensor(tokenized_seq[:-1], dtype=torch.long)\n",
    "        target_ids = torch.tensor(tokenized_seq[1:], dtype=torch.long)\n",
    "\n",
    "        return input_ids, target_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713c4694",
   "metadata": {},
   "source": [
    "We will also need to implement the associated dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "03d399be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = NamesDataset(\n",
    "    train=True\n",
    ")\n",
    "\n",
    "test_dataset = NamesDataset(\n",
    "    train=False\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acfe1de",
   "metadata": {},
   "source": [
    "### V. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7629a384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING DEVICE: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # Apple Silicon GPU\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "print(f\"USING DEVICE: {device}\")\n",
    "\n",
    "model = StandaloneDecoder(\n",
    "    trg_vocab_size=len(train_dataset.vocab),\n",
    "    embed_size=EMBED_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    num_heads=4,\n",
    "    forward_expansion=FORWARD_EXPANSION,\n",
    "    dropout=DROPOUT,\n",
    "    device=device,\n",
    "    max_length=MAX_LENGTH\n",
    ")\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bc960830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# for epoch in range(NUM_EPOCHS):\n",
    "#     print(f\"\\n EPOCH {epoch+1}/{NUM_EPOCHS}\")\n",
    "\n",
    "#     #########################\n",
    "#     ##### Train epoch: ######\n",
    "#     #########################\n",
    "#     model.train()\n",
    "\n",
    "#     for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "#         total_loss = 0\n",
    "    \n",
    "#     for batch_idx, (input_ids, target_ids) in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
    "#         input_ids = input_ids.to(device)\n",
    "#         target_ids = target_ids.to(device)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         # Forward pass\n",
    "#         logits = model(input_ids)  # (batch_size, seq_len, vocab_size)\n",
    "        \n",
    "#         # Reshape pour CrossEntropyLoss\n",
    "#         loss = criterion(logits.reshape(-1, logits.size(-1)), target_ids.reshape(-1))\n",
    "        \n",
    "#         # Backward pass\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         total_loss += loss.item()\n",
    "        \n",
    "#         # Log p√©riodique\n",
    "#         if (batch_idx + 1) % 50 == 0:\n",
    "#             avg_loss = total_loss / (batch_idx + 1)\n",
    "#             print(f\"  Batch {batch_idx+1}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "#     print(f\"‚úÖ Epoch {epoch+1} termin√©e! Loss moyenne: {total_loss/len(train_loader):.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6e3f7ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ === ENTRA√éNEMENT AVEC TESTS INT√âGR√âS ===\n",
      "\n",
      "============================================================\n",
      "üöÇ EPOCH 1/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üèãÔ∏è Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 203/203 [00:05<00:00, 35.19it/s]\n",
      "üß™ Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:00<00:00, 155.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ === R√âSULTATS EPOCH 1 ===\n",
      "üìä Train Loss: 1.0115\n",
      "üìä Test Loss:  0.8911\n",
      "üéØ Accuracy:   26.05%\n",
      "\n",
      "üé® === G√âN√âRATION DE NOMS ===\n",
      "üî§ G√©n√©ration √† partir de <SOS>:\n",
      "   ‚Ä¢ Nom 1: 'amaran' (longueur: 6)\n",
      "   ‚Ä¢ Nom 2: 'tulnha' (longueur: 6)\n",
      "   ‚Ä¢ Nom 3: 'deli' (longueur: 4)\n",
      "   ‚Ä¢ Nom 4: 'jenho' (longueur: 5)\n",
      "   ‚Ä¢ Nom 5: 'eropina' (longueur: 7)\n",
      "\n",
      "üîç Compl√©tion de pr√©fixes:\n",
      "   ‚Ä¢ 'a' ‚Üí 'arinda'\n",
      "   ‚Ä¢ 'm' ‚Üí 'mondin'\n",
      "   ‚Ä¢ 'j' ‚Üí 'jinan'\n",
      "   ‚Ä¢ 'l' ‚Üí 'larher'\n",
      "   ‚Ä¢ 'c' ‚Üí 'cranil'\n",
      "\n",
      "üìà === ANALYSE DES PR√âDICTIONS ===\n",
      "   ‚Ä¢ Input: 'cedric' | Target: 'cedric' | Pred: 'mariina'\n",
      "   ‚Ä¢ Input: 'yvette' | Target: 'yvette' | Pred: 'maanian'\n",
      "   ‚Ä¢ Input: 'hedwig' | Target: 'hedwig' | Pred: 'marian'\n",
      "\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "üöÇ EPOCH 2/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üèãÔ∏è Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 203/203 [00:05<00:00, 36.46it/s]\n",
      "üß™ Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:00<00:00, 161.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ === R√âSULTATS EPOCH 2 ===\n",
      "üìä Train Loss: 0.9068\n",
      "üìä Test Loss:  0.8672\n",
      "üéØ Accuracy:   27.35%\n",
      "\n",
      "üé® === G√âN√âRATION DE NOMS ===\n",
      "üî§ G√©n√©ration √† partir de <SOS>:\n",
      "   ‚Ä¢ Nom 1: 'cavallice' (longueur: 9)\n",
      "   ‚Ä¢ Nom 2: 'shozia' (longueur: 6)\n",
      "   ‚Ä¢ Nom 3: 'frasenn' (longueur: 7)\n",
      "   ‚Ä¢ Nom 4: 'jabllola' (longueur: 8)\n",
      "   ‚Ä¢ Nom 5: 'rablia' (longueur: 6)\n",
      "\n",
      "üîç Compl√©tion de pr√©fixes:\n",
      "   ‚Ä¢ 'a' ‚Üí 'ashane'\n",
      "   ‚Ä¢ 'm' ‚Üí 'marty'\n",
      "   ‚Ä¢ 'j' ‚Üí 'janien'\n",
      "   ‚Ä¢ 'l' ‚Üí 'laris'\n",
      "   ‚Ä¢ 'c' ‚Üí 'challine'\n",
      "\n",
      "üìà === ANALYSE DES PR√âDICTIONS ===\n",
      "   ‚Ä¢ Input: 'cedric' | Target: 'cedric' | Pred: 'mhriinh'\n",
      "   ‚Ä¢ Input: 'yvette' | Target: 'yvette' | Pred: 'maarhir'\n",
      "   ‚Ä¢ Input: 'hedwig' | Target: 'hedwig' | Pred: 'marian'\n",
      "\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "üöÇ EPOCH 3/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üèãÔ∏è Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 203/203 [00:05<00:00, 35.78it/s]\n",
      "üß™ Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:00<00:00, 150.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ === R√âSULTATS EPOCH 3 ===\n",
      "üìä Train Loss: 0.8885\n",
      "üìä Test Loss:  0.8584\n",
      "üéØ Accuracy:   27.51%\n",
      "\n",
      "üé® === G√âN√âRATION DE NOMS ===\n",
      "üî§ G√©n√©ration √† partir de <SOS>:\n",
      "   ‚Ä¢ Nom 1: 'wacan' (longueur: 5)\n",
      "   ‚Ä¢ Nom 2: 'iva' (longueur: 3)\n",
      "   ‚Ä¢ Nom 3: 'brag' (longueur: 4)\n",
      "   ‚Ä¢ Nom 4: 'adira' (longueur: 5)\n",
      "   ‚Ä¢ Nom 5: 'sachris' (longueur: 7)\n",
      "\n",
      "üîç Compl√©tion de pr√©fixes:\n",
      "   ‚Ä¢ 'a' ‚Üí 'ash'\n",
      "   ‚Ä¢ 'm' ‚Üí 'marin'\n",
      "   ‚Ä¢ 'j' ‚Üí 'jannuer'\n",
      "   ‚Ä¢ 'l' ‚Üí 'lanel'\n",
      "   ‚Ä¢ 'c' ‚Üí 'cavila'\n",
      "\n",
      "üìà === ANALYSE DES PR√âDICTIONS ===\n",
      "   ‚Ä¢ Input: 'cedric' | Target: 'cedric' | Pred: 'mariina'\n",
      "   ‚Ä¢ Input: 'yvette' | Target: 'yvette' | Pred: 'maartr'\n",
      "   ‚Ä¢ Input: 'hedwig' | Target: 'hedwig' | Pred: 'marien'\n",
      "\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "üöÇ EPOCH 4/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üèãÔ∏è Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 203/203 [00:05<00:00, 35.11it/s]\n",
      "üß™ Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:00<00:00, 152.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ === R√âSULTATS EPOCH 4 ===\n",
      "üìä Train Loss: 0.8741\n",
      "üìä Test Loss:  0.8549\n",
      "üéØ Accuracy:   26.30%\n",
      "\n",
      "üé® === G√âN√âRATION DE NOMS ===\n",
      "üî§ G√©n√©ration √† partir de <SOS>:\n",
      "   ‚Ä¢ Nom 1: 'sherinna' (longueur: 8)\n",
      "   ‚Ä¢ Nom 2: 'muariaka' (longueur: 8)\n",
      "   ‚Ä¢ Nom 3: 'corris' (longueur: 6)\n",
      "   ‚Ä¢ Nom 4: 'mayldre' (longueur: 7)\n",
      "   ‚Ä¢ Nom 5: 'marbrina' (longueur: 8)\n",
      "\n",
      "üîç Compl√©tion de pr√©fixes:\n",
      "   ‚Ä¢ 'a' ‚Üí 'alinna'\n",
      "   ‚Ä¢ 'm' ‚Üí 'marlie'\n",
      "   ‚Ä¢ 'j' ‚Üí 'jechel'\n",
      "   ‚Ä¢ 'l' ‚Üí 'linnis'\n",
      "   ‚Ä¢ 'c' ‚Üí 'carin'\n",
      "\n",
      "üìà === ANALYSE DES PR√âDICTIONS ===\n",
      "   ‚Ä¢ Input: 'cedric' | Target: 'cedric' | Pred: 'mhlrinh'\n",
      "   ‚Ä¢ Input: 'yvette' | Target: 'yvette' | Pred: 'maalhhl'\n",
      "   ‚Ä¢ Input: 'hedwig' | Target: 'hedwig' | Pred: 'marre'\n",
      "\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "üöÇ EPOCH 5/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üèãÔ∏è Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 203/203 [00:05<00:00, 36.44it/s]\n",
      "üß™ Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:00<00:00, 150.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ === R√âSULTATS EPOCH 5 ===\n",
      "üìä Train Loss: 0.8633\n",
      "üìä Test Loss:  0.8434\n",
      "üéØ Accuracy:   27.65%\n",
      "\n",
      "üé® === G√âN√âRATION DE NOMS ===\n",
      "üî§ G√©n√©ration √† partir de <SOS>:\n",
      "   ‚Ä¢ Nom 1: 'merli' (longueur: 5)\n",
      "   ‚Ä¢ Nom 2: 'danvi' (longueur: 5)\n",
      "   ‚Ä¢ Nom 3: 'onio' (longueur: 4)\n",
      "   ‚Ä¢ Nom 4: 'lika' (longueur: 4)\n",
      "   ‚Ä¢ Nom 5: 'sesana' (longueur: 6)\n",
      "\n",
      "üîç Compl√©tion de pr√©fixes:\n",
      "   ‚Ä¢ 'a' ‚Üí 'andia'\n",
      "   ‚Ä¢ 'm' ‚Üí 'mare'\n",
      "   ‚Ä¢ 'j' ‚Üí 'janana'\n",
      "   ‚Ä¢ 'l' ‚Üí 'liz'\n",
      "   ‚Ä¢ 'c' ‚Üí 'canilio'\n",
      "\n",
      "üìà === ANALYSE DES PR√âDICTIONS ===\n",
      "   ‚Ä¢ Input: 'cedric' | Target: 'cedric' | Pred: 'maliinh'\n",
      "   ‚Ä¢ Input: 'yvette' | Target: 'yvette' | Pred: 'maar'\n",
      "   ‚Ä¢ Input: 'hedwig' | Target: 'hedwig' | Pred: 'malian'\n",
      "\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "üöÇ EPOCH 6/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üèãÔ∏è Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 203/203 [00:05<00:00, 36.42it/s]\n",
      "üß™ Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:00<00:00, 162.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ === R√âSULTATS EPOCH 6 ===\n",
      "üìä Train Loss: 0.8540\n",
      "üìä Test Loss:  0.8420\n",
      "üéØ Accuracy:   28.27%\n",
      "\n",
      "üé® === G√âN√âRATION DE NOMS ===\n",
      "üî§ G√©n√©ration √† partir de <SOS>:\n",
      "   ‚Ä¢ Nom 1: 'beane' (longueur: 5)\n",
      "   ‚Ä¢ Nom 2: 'valerich' (longueur: 8)\n",
      "   ‚Ä¢ Nom 3: 'coil' (longueur: 4)\n",
      "   ‚Ä¢ Nom 4: 'goren' (longueur: 5)\n",
      "   ‚Ä¢ Nom 5: 'malemen' (longueur: 7)\n",
      "\n",
      "üîç Compl√©tion de pr√©fixes:\n",
      "   ‚Ä¢ 'a' ‚Üí 'aulia'\n",
      "   ‚Ä¢ 'm' ‚Üí 'marlen'\n",
      "   ‚Ä¢ 'j' ‚Üí 'joan'\n",
      "   ‚Ä¢ 'l' ‚Üí 'linei'\n",
      "   ‚Ä¢ 'c' ‚Üí 'carela'\n",
      "\n",
      "üìà === ANALYSE DES PR√âDICTIONS ===\n",
      "   ‚Ä¢ Input: 'cedric' | Target: 'cedric' | Pred: 'maliii'\n",
      "   ‚Ä¢ Input: 'yvette' | Target: 'yvette' | Pred: 'maant'\n",
      "   ‚Ä¢ Input: 'hedwig' | Target: 'hedwig' | Pred: 'marie'\n",
      "\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "üöÇ EPOCH 7/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üèãÔ∏è Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 203/203 [00:05<00:00, 37.33it/s]\n",
      "üß™ Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:00<00:00, 164.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ === R√âSULTATS EPOCH 7 ===\n",
      "üìä Train Loss: 0.8465\n",
      "üìä Test Loss:  0.8403\n",
      "üéØ Accuracy:   27.64%\n",
      "\n",
      "üé® === G√âN√âRATION DE NOMS ===\n",
      "üî§ G√©n√©ration √† partir de <SOS>:\n",
      "   ‚Ä¢ Nom 1: 'cronnne' (longueur: 7)\n",
      "   ‚Ä¢ Nom 2: 'chronn' (longueur: 6)\n",
      "   ‚Ä¢ Nom 3: 'allyndo' (longueur: 7)\n",
      "   ‚Ä¢ Nom 4: 'estian' (longueur: 6)\n",
      "   ‚Ä¢ Nom 5: 'katelven' (longueur: 8)\n",
      "\n",
      "üîç Compl√©tion de pr√©fixes:\n",
      "   ‚Ä¢ 'a' ‚Üí 'alin'\n",
      "   ‚Ä¢ 'm' ‚Üí 'meron'\n",
      "   ‚Ä¢ 'j' ‚Üí 'jara'\n",
      "   ‚Ä¢ 'l' ‚Üí 'lucing'\n",
      "   ‚Ä¢ 'c' ‚Üí 'cunal'\n",
      "\n",
      "üìà === ANALYSE DES PR√âDICTIONS ===\n",
      "   ‚Ä¢ Input: 'cedric' | Target: 'cedric' | Pred: 'chriino'\n",
      "   ‚Ä¢ Input: 'yvette' | Target: 'yvette' | Pred: 'caanhhn'\n",
      "   ‚Ä¢ Input: 'hedwig' | Target: 'hedwig' | Pred: 'carian'\n",
      "\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "üöÇ EPOCH 8/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üèãÔ∏è Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 203/203 [00:05<00:00, 37.36it/s]\n",
      "üß™ Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:00<00:00, 145.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ === R√âSULTATS EPOCH 8 ===\n",
      "üìä Train Loss: 0.8409\n",
      "üìä Test Loss:  0.8402\n",
      "üéØ Accuracy:   28.36%\n",
      "\n",
      "üé® === G√âN√âRATION DE NOMS ===\n",
      "üî§ G√©n√©ration √† partir de <SOS>:\n",
      "   ‚Ä¢ Nom 1: 'marlee' (longueur: 6)\n",
      "   ‚Ä¢ Nom 2: 'bauria' (longueur: 6)\n",
      "   ‚Ä¢ Nom 3: 'rabie' (longueur: 5)\n",
      "   ‚Ä¢ Nom 4: 'marhen' (longueur: 6)\n",
      "   ‚Ä¢ Nom 5: 'jufi' (longueur: 4)\n",
      "\n",
      "üîç Compl√©tion de pr√©fixes:\n",
      "   ‚Ä¢ 'a' ‚Üí 'alyster'\n",
      "   ‚Ä¢ 'm' ‚Üí 'marika'\n",
      "   ‚Ä¢ 'j' ‚Üí 'jeland'\n",
      "   ‚Ä¢ 'l' ‚Üí 'liki'\n",
      "   ‚Ä¢ 'c' ‚Üí 'chiliga'\n",
      "\n",
      "üìà === ANALYSE DES PR√âDICTIONS ===\n",
      "   ‚Ä¢ Input: 'cedric' | Target: 'cedric' | Pred: 'shliisi'\n",
      "   ‚Ä¢ Input: 'yvette' | Target: 'yvette' | Pred: 'saalt'\n",
      "   ‚Ä¢ Input: 'hedwig' | Target: 'hedwig' | Pred: 'sarii'\n",
      "\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "üöÇ EPOCH 9/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üèãÔ∏è Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 203/203 [00:05<00:00, 37.74it/s]\n",
      "üß™ Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:00<00:00, 166.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ === R√âSULTATS EPOCH 9 ===\n",
      "üìä Train Loss: 0.8363\n",
      "üìä Test Loss:  0.8261\n",
      "üéØ Accuracy:   29.42%\n",
      "\n",
      "üé® === G√âN√âRATION DE NOMS ===\n",
      "üî§ G√©n√©ration √† partir de <SOS>:\n",
      "   ‚Ä¢ Nom 1: 'perly' (longueur: 5)\n",
      "   ‚Ä¢ Nom 2: 'dabie' (longueur: 5)\n",
      "   ‚Ä¢ Nom 3: 'gosi' (longueur: 4)\n",
      "   ‚Ä¢ Nom 4: 'mari' (longueur: 4)\n",
      "   ‚Ä¢ Nom 5: 'allaw' (longueur: 5)\n",
      "\n",
      "üîç Compl√©tion de pr√©fixes:\n",
      "   ‚Ä¢ 'a' ‚Üí 'aman'\n",
      "   ‚Ä¢ 'm' ‚Üí 'memiki'\n",
      "   ‚Ä¢ 'j' ‚Üí 'janal'\n",
      "   ‚Ä¢ 'l' ‚Üí 'lima'\n",
      "   ‚Ä¢ 'c' ‚Üí 'chrick'\n",
      "\n",
      "üìà === ANALYSE DES PR√âDICTIONS ===\n",
      "   ‚Ä¢ Input: 'cedric' | Target: 'cedric' | Pred: 'mhliink'\n",
      "   ‚Ä¢ Input: 'yvette' | Target: 'yvette' | Pred: 'mainti'\n",
      "   ‚Ä¢ Input: 'hedwig' | Target: 'hedwig' | Pred: 'mariin'\n",
      "\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "üöÇ EPOCH 10/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üèãÔ∏è Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 203/203 [00:05<00:00, 38.18it/s]\n",
      "üß™ Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:00<00:00, 161.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ === R√âSULTATS EPOCH 10 ===\n",
      "üìä Train Loss: 0.8309\n",
      "üìä Test Loss:  0.8237\n",
      "üéØ Accuracy:   29.52%\n",
      "\n",
      "üé® === G√âN√âRATION DE NOMS ===\n",
      "üî§ G√©n√©ration √† partir de <SOS>:\n",
      "   ‚Ä¢ Nom 1: 'chelia' (longueur: 6)\n",
      "   ‚Ä¢ Nom 2: 'jaivie' (longueur: 6)\n",
      "   ‚Ä¢ Nom 3: 'pirmon' (longueur: 6)\n",
      "   ‚Ä¢ Nom 4: 'ruzi' (longueur: 4)\n",
      "   ‚Ä¢ Nom 5: 'manno' (longueur: 5)\n",
      "\n",
      "üîç Compl√©tion de pr√©fixes:\n",
      "   ‚Ä¢ 'a' ‚Üí 'alisha'\n",
      "   ‚Ä¢ 'm' ‚Üí 'margin'\n",
      "   ‚Ä¢ 'j' ‚Üí 'jakhen'\n",
      "   ‚Ä¢ 'l' ‚Üí 'lilie'\n",
      "   ‚Ä¢ 'c' ‚Üí 'comilo'\n",
      "\n",
      "üìà === ANALYSE DES PR√âDICTIONS ===\n",
      "   ‚Ä¢ Input: 'cedric' | Target: 'cedric' | Pred: 'saliine'\n",
      "   ‚Ä¢ Input: 'yvette' | Target: 'yvette' | Pred: 'saarter'\n",
      "   ‚Ä¢ Input: 'hedwig' | Target: 'hedwig' | Pred: 'salian'\n",
      "\n",
      "============================================================\n",
      "\n",
      "üéâ === ENTRA√éNEMENT TERMIN√â ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# üöÄ ENTRA√éNEMENT avec tests int√©gr√©s √† chaque epoch\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"üöÄ === ENTRA√éNEMENT AVEC TESTS INT√âGR√âS ===\")\n",
    "\n",
    "# Cr√©er les DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üöÇ EPOCH {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    #########################\n",
    "    ##### TRAIN PHASE #######\n",
    "    #########################\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_batches = 0\n",
    "    \n",
    "    for batch_idx, (input_ids, target_ids) in enumerate(tqdm(train_loader, desc=\"üèãÔ∏è Training\")):\n",
    "        input_ids = input_ids.to(device)\n",
    "        target_ids = target_ids.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(input_ids)\n",
    "        \n",
    "        # Loss\n",
    "        loss = criterion(logits.reshape(-1, logits.size(-1)), target_ids.reshape(-1))\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_batches += 1\n",
    "    \n",
    "    avg_train_loss = train_loss / train_batches\n",
    "    \n",
    "    #########################\n",
    "    ##### TEST PHASE ########\n",
    "    #########################\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_batches = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input_ids, target_ids in tqdm(test_loader, desc=\"üß™ Testing\"):\n",
    "            input_ids = input_ids.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(input_ids)\n",
    "            \n",
    "            # Loss\n",
    "            loss = criterion(logits.reshape(-1, logits.size(-1)), target_ids.reshape(-1))\n",
    "            test_loss += loss.item()\n",
    "            test_batches += 1\n",
    "            \n",
    "            # Accuracy (ignorer les PAD tokens)\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            mask = (target_ids != train_dataset.pad_idx)\n",
    "            correct_predictions += ((predictions == target_ids) * mask).sum().item()\n",
    "            total_predictions += mask.sum().item()\n",
    "    \n",
    "    avg_test_loss = test_loss / test_batches\n",
    "    accuracy = correct_predictions / total_predictions * 100\n",
    "    \n",
    "    #########################\n",
    "    ##### G√âN√âRATION TEST ###\n",
    "    #########################\n",
    "    print(f\"\\nüéØ === R√âSULTATS EPOCH {epoch+1} ===\")\n",
    "    print(f\"üìä Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"üìä Test Loss:  {avg_test_loss:.4f}\")\n",
    "    print(f\"üéØ Accuracy:   {accuracy:.2f}%\")\n",
    "    \n",
    "    # Test de g√©n√©ration de noms\n",
    "    print(f\"\\nüé® === G√âN√âRATION DE NOMS ===\")\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # M√©thode 1: G√©n√©ration √† partir de SOS\n",
    "        print(\"üî§ G√©n√©ration √† partir de <SOS>:\")\n",
    "        for i in range(5):\n",
    "            # Commencer avec SOS\n",
    "            generated = [train_dataset.sos_idx]\n",
    "            current_input = torch.tensor([generated], dtype=torch.long).to(device)\n",
    "            \n",
    "            # G√©n√©rer caract√®re par caract√®re\n",
    "            for _ in range(MAX_LENGTH-1):\n",
    "                logits = model(current_input)\n",
    "                \n",
    "                # Prendre le dernier token pr√©dit\n",
    "                last_logits = logits[0, -1, :]\n",
    "                \n",
    "                # Sampling avec temp√©rature pour plus de diversit√©\n",
    "                temperature = 0.8\n",
    "                probs = F.softmax(last_logits / temperature, dim=-1)\n",
    "                next_token = torch.multinomial(probs, 1).item()\n",
    "                \n",
    "                # Arr√™ter si on g√©n√®re EOS\n",
    "                if next_token == train_dataset.eos_idx:\n",
    "                    break\n",
    "                    \n",
    "                generated.append(next_token)\n",
    "                \n",
    "                # Mettre √† jour l'input (garder que les derniers tokens pour √©viter de d√©passer MAX_LENGTH)\n",
    "                current_input = torch.tensor([generated[-MAX_LENGTH:]], dtype=torch.long).to(device)\n",
    "            \n",
    "            # D√©coder le nom g√©n√©r√©\n",
    "            generated_name = \"\"\n",
    "            for token_idx in generated[1:]:  # Skip SOS\n",
    "                if token_idx == train_dataset.eos_idx:\n",
    "                    break\n",
    "                if token_idx < len(train_dataset.vocab):\n",
    "                    char = train_dataset.idx2vocab[token_idx]\n",
    "                    if char not in ['<PAD>', '<SOS>', '<EOS>']:\n",
    "                        generated_name += char\n",
    "            \n",
    "            print(f\"   ‚Ä¢ Nom {i+1}: '{generated_name}' (longueur: {len(generated_name)})\")\n",
    "    \n",
    "    # M√©thode 2: Compl√©tion de pr√©fixes\n",
    "    print(f\"\\nüîç Compl√©tion de pr√©fixes:\")\n",
    "    prefixes = ['a', 'm', 'j', 'l', 'c']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for prefix in prefixes:\n",
    "            # Encoder le pr√©fixe\n",
    "            prefix_tokens = [train_dataset.sos_idx]\n",
    "            for char in prefix.lower():\n",
    "                if char in train_dataset.vocab2idx:\n",
    "                    prefix_tokens.append(train_dataset.vocab2idx[char])\n",
    "            \n",
    "            if len(prefix_tokens) > 1:  # Si le pr√©fixe est valide\n",
    "                current_input = torch.tensor([prefix_tokens], dtype=torch.long).to(device)\n",
    "                \n",
    "                # Continuer la g√©n√©ration\n",
    "                for _ in range(MAX_LENGTH - len(prefix_tokens)):\n",
    "                    logits = model(current_input)\n",
    "                    last_logits = logits[0, -1, :]\n",
    "                    \n",
    "                    # Temp√©rature plus basse pour des compl√©tions plus coh√©rentes\n",
    "                    temperature = 0.6\n",
    "                    probs = F.softmax(last_logits / temperature, dim=-1)\n",
    "                    next_token = torch.multinomial(probs, 1).item()\n",
    "                    \n",
    "                    if next_token == train_dataset.eos_idx:\n",
    "                        break\n",
    "                    \n",
    "                    prefix_tokens.append(next_token)\n",
    "                    current_input = torch.tensor([prefix_tokens[-MAX_LENGTH:]], dtype=torch.long).to(device)\n",
    "                \n",
    "                # D√©coder\n",
    "                completed_name = \"\"\n",
    "                for token_idx in prefix_tokens[1:]:  # Skip SOS\n",
    "                    if token_idx == train_dataset.eos_idx:\n",
    "                        break\n",
    "                    if token_idx < len(train_dataset.vocab):\n",
    "                        char = train_dataset.idx2vocab[token_idx]\n",
    "                        if char not in ['<PAD>', '<SOS>', '<EOS>']:\n",
    "                            completed_name += char\n",
    "                \n",
    "                print(f\"   ‚Ä¢ '{prefix}' ‚Üí '{completed_name}'\")\n",
    "    \n",
    "    #########################\n",
    "    ##### ANALYSE PR√âDICTIONS\n",
    "    #########################\n",
    "    print(f\"\\nüìà === ANALYSE DES PR√âDICTIONS ===\")\n",
    "    \n",
    "    # Prendre quelques √©chantillons du test set\n",
    "    with torch.no_grad():\n",
    "        test_sample = next(iter(test_loader))\n",
    "        sample_inputs, sample_targets = test_sample\n",
    "        sample_inputs = sample_inputs[:3].to(device)  # 3 premiers √©chantillons\n",
    "        sample_targets = sample_targets[:3].to(device)\n",
    "        \n",
    "        logits = model(sample_inputs)\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        for i in range(3):\n",
    "            # D√©coder l'input (nom original)\n",
    "            input_name = \"\"\n",
    "            for token_idx in sample_inputs[i]:\n",
    "                if token_idx.item() < len(train_dataset.vocab):\n",
    "                    char = train_dataset.idx2vocab[token_idx.item()]\n",
    "                    if char not in ['<PAD>', '<SOS>', '<EOS>']:\n",
    "                        input_name += char\n",
    "            \n",
    "            # D√©coder la target\n",
    "            target_name = \"\"\n",
    "            for token_idx in sample_targets[i]:\n",
    "                if token_idx.item() != train_dataset.pad_idx:\n",
    "                    if token_idx.item() < len(train_dataset.vocab):\n",
    "                        char = train_dataset.idx2vocab[token_idx.item()]\n",
    "                        if char not in ['<PAD>', '<SOS>', '<EOS>']:\n",
    "                            target_name += char\n",
    "            \n",
    "            # D√©coder la pr√©diction\n",
    "            pred_name = \"\"\n",
    "            for token_idx in predictions[i]:\n",
    "                if token_idx.item() < len(train_dataset.vocab):\n",
    "                    char = train_dataset.idx2vocab[token_idx.item()]\n",
    "                    if char not in ['<PAD>', '<SOS>', '<EOS>']:\n",
    "                        pred_name += char\n",
    "            \n",
    "            print(f\"   ‚Ä¢ Input: '{input_name}' | Target: '{target_name}' | Pred: '{pred_name}'\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    \n",
    "    # Early stopping simple bas√© sur la loss\n",
    "    if epoch > 0 and avg_test_loss < 0.5:\n",
    "        print(f\"üéâ Convergence atteinte! Test loss < 0.5\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nüéâ === ENTRA√éNEMENT TERMIN√â ===\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
